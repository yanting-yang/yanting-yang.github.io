---
layout: post
title: "Multi-Head Attention"
mathjax: true
---

My learning notes for Multi-Head Attention.

## Why embedding dimension must be divided by number of heads?

In the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762):

> Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.
>
> $$
> \begin{align*}
>     \mathrm{MultiHead}(Q, K, V) &= \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O\\
>     \text{where}~\mathrm{head_i} &= \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\\
> \end{align*}
> $$
>
> Where the projections are parameter matrices $W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$ and $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$.

From the definition, there is no constraint on setting $h$ so it can be an arbitrary number.
However, in [torch.nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html):

> Note that `embed_dim` will be split across `num_heads` (i.e. each head will have dimension `embed_dim // num_heads`).

Why must `embed_dim` be divisible by `num_heads`?
